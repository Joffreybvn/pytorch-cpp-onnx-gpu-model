{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import h5py\n",
    "\n",
    "import onnx\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import tensorrt as trt\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_converted_model = \"../models/full_model.onnx\"\n",
    "path_dataset = \"../datasets/clean_dataset/dataset.h5py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5f = h5py.File(path_dataset, 'r')\n",
    "\n",
    "X_train = np.array(h5f['X_train'])\n",
    "X_test = np.array(h5f['X_test'])\n",
    "y_train = np.array(h5f['y_train'])\n",
    "y_test = np.array(h5f['y_test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to TensorRT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor RT engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRT_LOGGER = trt.Logger(trt.Logger.VERBOSE)\n",
    "flags = 1 << (int)(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)\n",
    "\n",
    "def build_engine(model_path):\n",
    "    \"\"\"Build and return the TensorRT engine and context.\"\"\"\n",
    "    \n",
    "    with trt.Builder(TRT_LOGGER) as builder, builder.create_network(flags) as network, trt.OnnxParser(network, TRT_LOGGER) as parser: \n",
    "        \n",
    "        # Set builder 1GB Vram, batch size 1 & fp16 if possible\n",
    "        builder.max_workspace_size = 1 << 30\n",
    "        builder.max_batch_size = 1\n",
    "        \n",
    "        if builder.platform_has_fast_fp16:\n",
    "            builder.fp16_mode = True\n",
    "        \n",
    "        # Load ONNX model\n",
    "        with open(model_path, \"rb\") as f:\n",
    "            parser.parse(f.read())\n",
    "        \n",
    "        # Generate an engine optimized for the target platform\n",
    "        engine = builder.build_cuda_engine(network)\n",
    "        context = engine.create_execution_context()\n",
    "        \n",
    "        print(engine)\n",
    "        print(context)\n",
    "\n",
    "        return engine, context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function allocate memory on the CPU and GPU. It has to be called before any inference. Once the memory has been allocated, the data can be passed and an inference can be done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allocate_buffer(engine):\n",
    "    \n",
    "    # Host CPU memory\n",
    "    h_in_size = trt.volume(engine.get_binding_shape(0))\n",
    "    h_out_size = trt.volume(engine.get_binding_shape(1))\n",
    "    h_in_dtype = trt.nptype(engine.get_binding_dtype(0))\n",
    "    h_out_dtype = trt.nptype(engine.get_binding_dtype(1))\n",
    "    \n",
    "    in_cpu = cuda.pagelocked_empty(h_in_size, h_in_dtype)\n",
    "    out_cpu = cuda.pagelocked_empty(h_out_size, h_out_dtype)\n",
    "    \n",
    "    # Allocate GPU memory\n",
    "    in_gpu = cuda.mem_alloc(in_cpu.nbytes)\n",
    "    out_gpu = cuda.mem_alloc(out_cpu.nbytes)\n",
    "    \n",
    "    stream = cuda.Stream()\n",
    "    return in_cpu, out_cpu, in_gpu, out_gpu, stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function take allocated space, an input matrix, and run the inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(engine, context, inputs, out_cpu, in_gpu, out_gpu, stream):\n",
    "    \n",
    "    cuda.memcpy_htod(in_gpu, inputs)\n",
    "    context.execute(1, [int(in_gpu), int(out_gpu)])\n",
    "    cuda.memcpy_dtoh(out_cpu, out_gpu)\n",
    "    \n",
    "    return out_cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tensorrt.tensorrt.ICudaEngine object at 0x7f363fe215a8>\n",
      "<tensorrt.tensorrt.IExecutionContext object at 0x7f363fe21d50>\n"
     ]
    }
   ],
   "source": [
    "inputs = np.random.random((1, 3, 120, 120)).astype(np.float32)\n",
    "engine, context = build_engine(path_converted_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.7634346 -0.3806094 -2.4297414 -2.8582964 -2.57013   -3.4568505]\n",
      "[-2.7634346 -0.3806094 -2.4297414 -2.8582964 -2.57013   -3.4568505]\n",
      "[-2.7634346 -0.3806094 -2.4297414 -2.8582964 -2.57013   -3.4568505]\n",
      "[-2.7634346 -0.3806094 -2.4297414 -2.8582964 -2.57013   -3.4568505]\n",
      "[-2.7634346 -0.3806094 -2.4297414 -2.8582964 -2.57013   -3.4568505]\n",
      "[-2.7634346 -0.3806094 -2.4297414 -2.8582964 -2.57013   -3.4568505]\n",
      "[-2.7634346 -0.3806094 -2.4297414 -2.8582964 -2.57013   -3.4568505]\n",
      "[-2.7634346 -0.3806094 -2.4297414 -2.8582964 -2.57013   -3.4568505]\n",
      "[-2.7634346 -0.3806094 -2.4297414 -2.8582964 -2.57013   -3.4568505]\n",
      "[-2.7634346 -0.3806094 -2.4297414 -2.8582964 -2.57013   -3.4568505]\n",
      "cost time:  0.029569625854492188\n"
     ]
    }
   ],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "for _ in range(10):\n",
    "    \n",
    "    in_cpu, out_cpu, in_gpu, out_gpu, stream = allocate_buffer(engine)\n",
    "    res = inference(engine, context, inputs.reshape(-1), out_cpu, in_gpu, out_gpu, stream)\n",
    "    print(res)\n",
    "\n",
    "print(\"cost time: \", time.time()-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}